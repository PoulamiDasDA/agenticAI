{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8811eca1",
   "metadata": {},
   "source": [
    "# Azure AI Search with Multiple Text Files\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Connect to Azure Storage Account\n",
    "2. Load data from multiple .txt files in a container\n",
    "3. Process and chunk text data\n",
    "4. Create and populate Azure AI Search index\n",
    "5. Set up knowledge agent for text retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e493ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install azure-storage-blob azure-search-documents azure-ai-projects azure-identity python-dotenv requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e01f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex, SearchField, VectorSearch, VectorSearchProfile, \n",
    "    HnswAlgorithmConfiguration, AzureOpenAIVectorizer, AzureOpenAIVectorizerParameters,\n",
    "    SemanticSearch, SemanticConfiguration, SemanticPrioritizedFields, SemanticField,\n",
    "    KnowledgeAgent, KnowledgeAgentAzureOpenAIModel, KnowledgeAgentTargetIndex, KnowledgeAgentRequestLimits\n",
    ")\n",
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae444d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Using Azure Search API Key authentication\n",
      "✅ Configuration loaded successfully\n",
      "Storage Account: storageagenticaidemo\n",
      "Container: earthdata\n",
      "File Prefix: All .txt files\n",
      "Search Index: txt_files_index\n",
      "Agent Name: txt-files-agent\n",
      "Chunk Size: 1000 characters\n",
      "Chunk Overlap: 200 characters\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Azure Storage Configuration for TXT files\n",
    "txt_storage_account_name = os.getenv(\"TXT_STORAGE_ACCOUNT_NAME\", os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\", \"your-storage-account\"))\n",
    "txt_storage_account_key = os.getenv(\"TXT_STORAGE_ACCOUNT_KEY\", os.getenv(\"AZURE_STORAGE_ACCOUNT_KEY\"))  \n",
    "txt_storage_container_name = os.getenv(\"TXT_STORAGE_CONTAINER_NAME\", \"txtfiles\")  # Different container for txt files\n",
    "txt_file_prefix = os.getenv(\"TXT_FILE_PREFIX\", \"\")  # Optional: filter files by prefix\n",
    "\n",
    "# Text Processing Configuration\n",
    "chunk_size = int(os.getenv(\"TEXT_CHUNK_SIZE\", \"1000\"))  # Characters per chunk\n",
    "chunk_overlap = int(os.getenv(\"TEXT_CHUNK_OVERLAP\", \"200\"))  # Overlap between chunks\n",
    "\n",
    "# Azure Search Configuration\n",
    "search_endpoint = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
    "search_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "\n",
    "# Use API key if available, otherwise use managed identity\n",
    "if search_api_key:\n",
    "    print(\"🔑 Using Azure Search API Key authentication\")\n",
    "    search_credential = AzureKeyCredential(search_api_key)\n",
    "else:\n",
    "    print(\"🔐 Using Managed Identity authentication\")\n",
    "    managed_identity_client_id = os.getenv(\"MANAGED_IDENTITY_CLIENT_ID\")\n",
    "    search_credential = DefaultAzureCredential(managed_identity_client_id=managed_identity_client_id)\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_gpt_deployment = os.getenv(\"AZURE_OPENAI_GPT_DEPLOYMENT\", \"gpt-4o\")\n",
    "azure_openai_gpt_model = os.getenv(\"AZURE_OPENAI_GPT_MODEL\", \"gpt-4o\")\n",
    "azure_openai_embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-large\")\n",
    "azure_openai_embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-large\")\n",
    "\n",
    "# General Configuration\n",
    "index_name = os.getenv(\"TXT_SEARCH_INDEX\", \"txt_files_index\")\n",
    "agent_name = os.getenv(\"TXT_SEARCH_AGENT_NAME\", \"txt-files-agent\")\n",
    "\n",
    "print(\"✅ Configuration loaded successfully\")\n",
    "print(f\"Storage Account: {txt_storage_account_name}\")\n",
    "print(f\"Container: {txt_storage_container_name}\")\n",
    "print(f\"File Prefix: {txt_file_prefix or 'All .txt files'}\")\n",
    "print(f\"Search Index: {index_name}\")\n",
    "print(f\"Agent Name: {agent_name}\")\n",
    "print(f\"Chunk Size: {chunk_size} characters\")\n",
    "print(f\"Chunk Overlap: {chunk_overlap} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33c725d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Connecting with Account Key...\n",
      "✅ Connected to Azure Storage successfully\n",
      "📦 Found 2 containers:\n",
      "   - demo\n",
      "   - earthdata\n",
      "✅ Target container 'earthdata' found\n"
     ]
    }
   ],
   "source": [
    "# Connect to Azure Storage\n",
    "def connect_to_txt_storage():\n",
    "    try:\n",
    "        if txt_storage_account_key:\n",
    "            print(\"🔑 Connecting with Account Key...\")\n",
    "            connection_string = f\"DefaultEndpointsProtocol=https;AccountName={txt_storage_account_name};AccountKey={txt_storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "            return BlobServiceClient.from_connection_string(connection_string)\n",
    "        else:\n",
    "            print(\"🔐 Connecting with Managed Identity...\")\n",
    "            account_url = f\"https://{txt_storage_account_name}.blob.core.windows.net\"\n",
    "            credential = DefaultAzureCredential()\n",
    "            return BlobServiceClient(account_url=account_url, credential=credential)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to connect to storage: {e}\")\n",
    "        return None\n",
    "\n",
    "# Establish connection\n",
    "txt_blob_service_client = connect_to_txt_storage()\n",
    "\n",
    "if txt_blob_service_client:\n",
    "    print(\"✅ Connected to Azure Storage successfully\")\n",
    "    \n",
    "    # Test connection by listing containers\n",
    "    try:\n",
    "        containers = list(txt_blob_service_client.list_containers())\n",
    "        print(f\"📦 Found {len(containers)} containers:\")\n",
    "        for container in containers[:5]:\n",
    "            print(f\"   - {container.name}\")\n",
    "            \n",
    "        # Check if our target container exists\n",
    "        container_names = [c.name for c in containers]\n",
    "        if txt_storage_container_name in container_names:\n",
    "            print(f\"✅ Target container '{txt_storage_container_name}' found\")\n",
    "        else:\n",
    "            print(f\"⚠️  Target container '{txt_storage_container_name}' not found\")\n",
    "            print(f\"Available containers: {container_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Could not list containers: {e}\")\n",
    "else:\n",
    "    print(\"❌ Failed to connect to storage. Please check your credentials.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5b81c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Scanning container 'earthdata' for .txt files...\n",
      "✅ Found 5 .txt files\n",
      "   📄 Earth_At_Night_Overview.txt (1.2 KB)\n",
      "   📄 Earth_Night_Ecosystems.txt (0.9 KB)\n",
      "   📄 Global_Landscape_Earth_At_Night.txt (3.0 KB)\n",
      "   📄 Human_Activity_At_Night.txt (0.8 KB)\n",
      "   📄 Night_Imagery_Disaster_Monitoring.txt (0.9 KB)\n"
     ]
    }
   ],
   "source": [
    "# List and load all .txt files from container\n",
    "def list_txt_files():\n",
    "    \"\"\"List all .txt files in the container\"\"\"\n",
    "    try:\n",
    "        container_client = txt_blob_service_client.get_container_client(txt_storage_container_name)\n",
    "        txt_files = []\n",
    "        \n",
    "        print(f\"📄 Scanning container '{txt_storage_container_name}' for .txt files...\")\n",
    "        \n",
    "        for blob in container_client.list_blobs():\n",
    "            # Filter by .txt extension and optional prefix\n",
    "            if blob.name.lower().endswith('.txt'):\n",
    "                if not txt_file_prefix or blob.name.startswith(txt_file_prefix):\n",
    "                    txt_files.append({\n",
    "                        'name': blob.name,\n",
    "                        'size': blob.size,\n",
    "                        'last_modified': blob.last_modified\n",
    "                    })\n",
    "        \n",
    "        print(f\"✅ Found {len(txt_files)} .txt files\")\n",
    "        \n",
    "        # Display file summary\n",
    "        for file in txt_files[:10]:  # Show first 10 files\n",
    "            size_kb = file['size'] / 1024\n",
    "            print(f\"   📄 {file['name']} ({size_kb:.1f} KB)\")\n",
    "        \n",
    "        if len(txt_files) > 10:\n",
    "            print(f\"   ... and {len(txt_files) - 10} more files\")\n",
    "        \n",
    "        return txt_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error listing .txt files: {e}\")\n",
    "        return []\n",
    "\n",
    "# Get list of txt files\n",
    "txt_files = list_txt_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "062b0c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text processing functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Text chunking functions\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text content\"\"\"\n",
    "    # Remove extra whitespace and normalize line endings\n",
    "    text = re.sub(r'\\r\\n', '\\n', text)\n",
    "    text = re.sub(r'\\r', '\\n', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Normalize paragraph breaks\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Normalize spaces\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Calculate end position\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        # If this is not the last chunk, try to break at sentence or word boundary\n",
    "        if end < len(text):\n",
    "            # Look for sentence endings within the last 100 characters\n",
    "            boundary_search = text[max(0, end-100):end+100]\n",
    "            sentence_ends = [m.end() for m in re.finditer(r'[.!?]\\s+', boundary_search)]\n",
    "            \n",
    "            if sentence_ends:\n",
    "                # Use the last sentence ending within our search area\n",
    "                relative_pos = sentence_ends[-1]\n",
    "                end = max(0, end-100) + relative_pos\n",
    "            else:\n",
    "                # Fall back to word boundary\n",
    "                while end < len(text) and text[end] != ' ':\n",
    "                    end += 1\n",
    "        \n",
    "        # Extract chunk\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # Move start position (with overlap)\n",
    "        start = end - overlap\n",
    "        if start >= len(text):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def generate_chunk_id(filename, chunk_index, chunk_text):\n",
    "    \"\"\"Generate unique ID for each chunk\"\"\"\n",
    "    # Create a hash of the content for uniqueness\n",
    "    content_hash = hashlib.md5(chunk_text.encode()).hexdigest()[:8]\n",
    "    # Clean filename for use in ID\n",
    "    clean_filename = re.sub(r'[^a-zA-Z0-9_-]', '_', filename.replace('.txt', ''))\n",
    "    return f\"{clean_filename}_chunk_{chunk_index:04d}_{content_hash}\"\n",
    "\n",
    "print(\"✅ Text processing functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62788d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Processing 5 text files...\n",
      "\n",
      "📄 Processing file 1/5: Earth_At_Night_Overview.txt\n",
      "   📏 Text length: 1,203 characters\n",
      "   ✂️  Created 2 chunks\n",
      "\n",
      "📄 Processing file 2/5: Earth_Night_Ecosystems.txt\n",
      "   📏 Text length: 883 characters\n",
      "   ✂️  Created 1 chunks\n",
      "\n",
      "📄 Processing file 3/5: Global_Landscape_Earth_At_Night.txt\n",
      "   📏 Text length: 3,013 characters\n",
      "   ✂️  Created 4 chunks\n",
      "\n",
      "📄 Processing file 4/5: Human_Activity_At_Night.txt\n",
      "   📏 Text length: 781 characters\n",
      "   ✂️  Created 1 chunks\n",
      "\n",
      "📄 Processing file 5/5: Night_Imagery_Disaster_Monitoring.txt\n",
      "   📏 Text length: 884 characters\n",
      "   ✂️  Created 1 chunks\n",
      "\n",
      "✅ Successfully processed 5 files into 9 searchable documents\n",
      "\n",
      "📊 Processing Summary:\n",
      "   📄 Files processed: 5\n",
      "   📋 Total chunks: 9\n",
      "   📏 Total characters: 7,557\n",
      "   📐 Average chunk size: 840 characters\n",
      "\n",
      "📝 Sample document:\n",
      "   ID: Earth_At_Night_Overview_chunk_0000_54014950\n",
      "   Source: Earth_At_Night_Overview.txt\n",
      "   Content preview: Title: Earth at Night - A Scientific Overview\n",
      "\n",
      "The phenomenon of Earth at night offers a fascinating view into both natural and artificial illumination across the globe.\n",
      "From space, astronauts aboard ...\n"
     ]
    }
   ],
   "source": [
    "# Load and process all text files\n",
    "def load_and_process_txt_files():\n",
    "    \"\"\"Load all txt files and convert to searchable documents\"\"\"\n",
    "    if not txt_files:\n",
    "        print(\"❌ No .txt files found to process\")\n",
    "        return []\n",
    "    \n",
    "    all_documents = []\n",
    "    total_files = len(txt_files)\n",
    "    \n",
    "    print(f\"📥 Processing {total_files} text files...\")\n",
    "    \n",
    "    for i, file_info in enumerate(txt_files, 1):\n",
    "        filename = file_info['name']\n",
    "        print(f\"\\n📄 Processing file {i}/{total_files}: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Download file content\n",
    "            blob_client = txt_blob_service_client.get_blob_client(\n",
    "                container=txt_storage_container_name,\n",
    "                blob=filename\n",
    "            )\n",
    "            \n",
    "            # Get text content\n",
    "            blob_data = blob_client.download_blob()\n",
    "            raw_text = blob_data.readall().decode('utf-8', errors='ignore')\n",
    "            \n",
    "            # Clean the text\n",
    "            cleaned_text = clean_text(raw_text)\n",
    "            \n",
    "            print(f\"   📏 Text length: {len(cleaned_text):,} characters\")\n",
    "            \n",
    "            # Split into chunks\n",
    "            chunks = chunk_text(cleaned_text, chunk_size, chunk_overlap)\n",
    "            print(f\"   ✂️  Created {len(chunks)} chunks\")\n",
    "            \n",
    "            # Convert chunks to documents\n",
    "            for chunk_idx, chunk_content in enumerate(chunks):\n",
    "                doc = {\n",
    "                    'id': generate_chunk_id(filename, chunk_idx, chunk_content),\n",
    "                    'content': chunk_content,\n",
    "                    'source_file': filename,\n",
    "                    'chunk_index': chunk_idx,\n",
    "                    'file_size': file_info['size'],\n",
    "                    'file_modified': file_info['last_modified'].isoformat() if file_info['last_modified'] else None,\n",
    "                    'processed_date': datetime.now().isoformat(),\n",
    "                    'chunk_length': len(chunk_content),\n",
    "                    'file_type': 'text'\n",
    "                }\n",
    "                all_documents.append(doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✅ Successfully processed {total_files} files into {len(all_documents)} searchable documents\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    if all_documents:\n",
    "        total_chars = sum(doc['chunk_length'] for doc in all_documents)\n",
    "        avg_chunk_size = total_chars / len(all_documents)\n",
    "        unique_files = len(set(doc['source_file'] for doc in all_documents))\n",
    "        \n",
    "        print(f\"\\n📊 Processing Summary:\")\n",
    "        print(f\"   📄 Files processed: {unique_files}\")\n",
    "        print(f\"   📋 Total chunks: {len(all_documents)}\")\n",
    "        print(f\"   📏 Total characters: {total_chars:,}\")\n",
    "        print(f\"   📐 Average chunk size: {avg_chunk_size:.0f} characters\")\n",
    "        \n",
    "        # Show sample document\n",
    "        print(f\"\\n📝 Sample document:\")\n",
    "        sample_doc = all_documents[0]\n",
    "        print(f\"   ID: {sample_doc['id']}\")\n",
    "        print(f\"   Source: {sample_doc['source_file']}\")\n",
    "        print(f\"   Content preview: {sample_doc['content'][:200]}...\")\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "# Process all files\n",
    "documents = load_and_process_txt_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa22e54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Creating search index: txt_files_index\n",
      "✅ Index 'txt_files_index' created or updated successfully\n"
     ]
    }
   ],
   "source": [
    "# Create Azure AI Search Index for text documents\n",
    "def create_txt_search_index():\n",
    "    try:\n",
    "        print(f\"🔍 Creating search index: {index_name}\")\n",
    "        \n",
    "        index = SearchIndex(\n",
    "            name=index_name,\n",
    "            fields=[\n",
    "                # Required key field\n",
    "                SearchField(name=\"id\", type=\"Edm.String\", key=True, filterable=True, sortable=True),\n",
    "                \n",
    "                # Main content field\n",
    "                SearchField(name=\"content\", type=\"Edm.String\", searchable=True, filterable=False),\n",
    "                \n",
    "                # Vector embedding field\n",
    "                SearchField(name=\"content_vector\", type=\"Collection(Edm.Single)\", stored=False, \n",
    "                           vector_search_dimensions=3072, vector_search_profile_name=\"hnsw_text_3_large\"),\n",
    "                \n",
    "                # Metadata fields\n",
    "                SearchField(name=\"source_file\", type=\"Edm.String\", filterable=True, sortable=True, facetable=True),\n",
    "                SearchField(name=\"chunk_index\", type=\"Edm.Int32\", filterable=True, sortable=True),\n",
    "                SearchField(name=\"file_size\", type=\"Edm.Int64\", filterable=True, sortable=True),\n",
    "                SearchField(name=\"file_modified\", type=\"Edm.DateTimeOffset\", filterable=True, sortable=True),\n",
    "                SearchField(name=\"processed_date\", type=\"Edm.DateTimeOffset\", filterable=True, sortable=True),\n",
    "                SearchField(name=\"chunk_length\", type=\"Edm.Int32\", filterable=True, sortable=True),\n",
    "                SearchField(name=\"file_type\", type=\"Edm.String\", filterable=True, facetable=True)\n",
    "            ],\n",
    "            vector_search=VectorSearch(\n",
    "                profiles=[\n",
    "                    VectorSearchProfile(\n",
    "                        name=\"hnsw_text_3_large\", \n",
    "                        algorithm_configuration_name=\"alg\", \n",
    "                        vectorizer_name=\"azure_openai_text_3_large\"\n",
    "                    )\n",
    "                ],\n",
    "                algorithms=[HnswAlgorithmConfiguration(name=\"alg\")],\n",
    "                vectorizers=[\n",
    "                    AzureOpenAIVectorizer(\n",
    "                        vectorizer_name=\"azure_openai_text_3_large\",\n",
    "                        parameters=AzureOpenAIVectorizerParameters(\n",
    "                            resource_url=azure_openai_endpoint,\n",
    "                            deployment_name=azure_openai_embedding_deployment,\n",
    "                            model_name=azure_openai_embedding_model\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            semantic_search=SemanticSearch(\n",
    "                default_configuration_name=\"semantic_config\",\n",
    "                configurations=[\n",
    "                    SemanticConfiguration(\n",
    "                        name=\"semantic_config\",\n",
    "                        prioritized_fields=SemanticPrioritizedFields(\n",
    "                            content_fields=[\n",
    "                                SemanticField(field_name=\"content\")\n",
    "                            ],\n",
    "                            keywords_fields=[\n",
    "                                SemanticField(field_name=\"source_file\")\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        index_client = SearchIndexClient(endpoint=search_endpoint, credential=search_credential)\n",
    "        result = index_client.create_or_update_index(index)\n",
    "        print(f\"✅ Index '{index_name}' created or updated successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create index: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create the index\n",
    "index_created = create_txt_search_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbf7f1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Uploading 9 documents to index...\n",
      "   📦 Processing batch 1/1 (9 documents)\n",
      "✅ All 9 documents uploaded to index 'txt_files_index' successfully\n"
     ]
    }
   ],
   "source": [
    "# Upload documents to the search index\n",
    "def upload_txt_documents_to_index():\n",
    "    if not documents:\n",
    "        print(\"❌ No documents to upload\")\n",
    "        return False\n",
    "        \n",
    "    if not index_created:\n",
    "        print(\"❌ Index not created, cannot upload documents\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(f\"📤 Uploading {len(documents)} documents to index...\")\n",
    "        \n",
    "        # Process documents in batches for better performance\n",
    "        batch_size = 100\n",
    "        total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "        \n",
    "        with SearchIndexingBufferedSender(\n",
    "            endpoint=search_endpoint, \n",
    "            index_name=index_name, \n",
    "            credential=search_credential,\n",
    "            auto_flush_interval=30  # Auto-flush every 30 seconds\n",
    "        ) as client:\n",
    "            for i in range(0, len(documents), batch_size):\n",
    "                batch = documents[i:i + batch_size]\n",
    "                batch_num = (i // batch_size) + 1\n",
    "                \n",
    "                print(f\"   📦 Processing batch {batch_num}/{total_batches} ({len(batch)} documents)\")\n",
    "                client.upload_documents(documents=batch)\n",
    "        \n",
    "        print(f\"✅ All {len(documents)} documents uploaded to index '{index_name}' successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to upload documents: {e}\")\n",
    "        return False\n",
    "\n",
    "# Upload the documents\n",
    "documents_uploaded = upload_txt_documents_to_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2af39e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Creating knowledge agent: txt-files-agent\n",
      "✅ Knowledge agent 'txt-files-agent' created or updated successfully\n"
     ]
    }
   ],
   "source": [
    "# Create Knowledge Agent for text files\n",
    "def create_txt_knowledge_agent():\n",
    "    if not documents_uploaded:\n",
    "        print(\"❌ Documents not uploaded, cannot create agent\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(f\"🤖 Creating knowledge agent: {agent_name}\")\n",
    "        \n",
    "        agent = KnowledgeAgent(\n",
    "            name=agent_name,\n",
    "            models=[\n",
    "                KnowledgeAgentAzureOpenAIModel(\n",
    "                    azure_open_ai_parameters=AzureOpenAIVectorizerParameters(\n",
    "                        resource_url=azure_openai_endpoint,\n",
    "                        deployment_name=azure_openai_gpt_deployment,\n",
    "                        model_name=azure_openai_gpt_model\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            target_indexes=[\n",
    "                KnowledgeAgentTargetIndex(\n",
    "                    index_name=index_name,\n",
    "                    default_reranker_threshold=2.0  # Slightly lower threshold for text similarity\n",
    "                )\n",
    "            ],\n",
    "            request_limits=KnowledgeAgentRequestLimits(\n",
    "                max_output_size=15000  # Larger output for text processing\n",
    "            )\n",
    "        )\n",
    "\n",
    "        index_client = SearchIndexClient(endpoint=search_endpoint, credential=search_credential)\n",
    "        result = index_client.create_or_update_agent(agent)\n",
    "        print(f\"✅ Knowledge agent '{agent_name}' created or updated successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create knowledge agent: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create the agent\n",
    "agent_created = create_txt_knowledge_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca2a8733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing text search functionality...\n",
      "\n",
      "🔎 Test 1: Simple text search\n",
      "\n",
      "🎯 Search Results for 'important information':\n",
      "\n",
      "🔎 Test 2: Search with file filter\n",
      "\n",
      "📄 Results from file 'Earth_At_Night_Overview.txt':\n",
      "\n",
      "🔎 Test 3: Document statistics\n",
      "\n",
      "📊 Documents per file:\n"
     ]
    }
   ],
   "source": [
    "# Test the setup with multiple search scenarios\n",
    "def test_txt_search_functionality():\n",
    "    try:\n",
    "        from azure.search.documents import SearchClient\n",
    "        \n",
    "        print(\"🔍 Testing text search functionality...\")\n",
    "        \n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=search_credential\n",
    "        )\n",
    "        \n",
    "        # Test 1: Simple text search\n",
    "        print(\"\\n🔎 Test 1: Simple text search\")\n",
    "        results = search_client.search(\"important information\", top=3)\n",
    "        \n",
    "        print(f\"\\n🎯 Search Results for 'important information':\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\nResult {i}:\")\n",
    "            print(f\"   File: {result.get('source_file', 'N/A')}\")\n",
    "            print(f\"   Chunk: {result.get('chunk_index', 'N/A')}\")\n",
    "            print(f\"   Content: {result.get('content', 'N/A')[:150]}...\")\n",
    "            print(f\"   Score: {result.get('@search.score', 'N/A'):.3f}\")\n",
    "        \n",
    "        # Test 2: Search with file filter\n",
    "        if len(txt_files) > 1:\n",
    "            print(\"\\n🔎 Test 2: Search with file filter\")\n",
    "            first_file = txt_files[0]['name']\n",
    "            results = search_client.search(\n",
    "                \"*\", \n",
    "                top=2,\n",
    "                filter=f\"source_file eq '{first_file}'\"\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n📄 Results from file '{first_file}':\")\n",
    "            for i, result in enumerate(results, 1):\n",
    "                print(f\"\\nResult {i}:\")\n",
    "                print(f\"   Chunk: {result.get('chunk_index', 'N/A')}\")\n",
    "                print(f\"   Length: {result.get('chunk_length', 'N/A')} chars\")\n",
    "                print(f\"   Content: {result.get('content', 'N/A')[:100]}...\")\n",
    "        \n",
    "        # Test 3: Faceted search by file type\n",
    "        print(\"\\n🔎 Test 3: Document statistics\")\n",
    "        results = search_client.search(\n",
    "            \"*\", \n",
    "            top=0,  # We only want facets\n",
    "            facets=[\"source_file\", \"file_type\"]\n",
    "        )\n",
    "        \n",
    "        facets = results.get_facets()\n",
    "        if 'source_file' in facets:\n",
    "            print(\"\\n📊 Documents per file:\")\n",
    "            for facet in facets['source_file'][:5]:  # Top 5 files\n",
    "                print(f\"   📄 {facet['value']}: {facet['count']} chunks\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Search test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run search tests\n",
    "if documents_uploaded:\n",
    "    test_txt_search_functionality()\n",
    "else:\n",
    "    print(\"⏩ Skipping search test - documents not uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57a5a8",
   "metadata": {},
   "source": [
    "## 🎯 Summary\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "1. **Connect to Azure Storage** and scan for .txt files\n",
    "2. **Load and process multiple text files** from a container\n",
    "3. **Intelligently chunk text** with proper boundaries\n",
    "4. **Create Azure AI Search index** optimized for text content\n",
    "5. **Upload processed text chunks** as searchable documents\n",
    "6. **Create a knowledge agent** for text retrieval\n",
    "7. **Test search functionality** with various filters\n",
    "\n",
    "## 📝 Required Environment Variables\n",
    "\n",
    "Add these to your `.env` file:\n",
    "\n",
    "```env\n",
    "# Text Files Storage (can reuse existing storage account)\n",
    "TXT_STORAGE_ACCOUNT_NAME=your-storage-account\n",
    "TXT_STORAGE_ACCOUNT_KEY=your-storage-key\n",
    "TXT_STORAGE_CONTAINER_NAME=txtfiles\n",
    "TXT_FILE_PREFIX=documents/  # Optional: filter files by prefix\n",
    "\n",
    "# Text Processing\n",
    "TEXT_CHUNK_SIZE=1000\n",
    "TEXT_CHUNK_OVERLAP=200\n",
    "\n",
    "# Search Configuration\n",
    "TXT_SEARCH_INDEX=txt_files_index\n",
    "TXT_SEARCH_AGENT_NAME=txt-files-agent\n",
    "\n",
    "# Azure Services (existing)\n",
    "AZURE_SEARCH_ENDPOINT=your-search-endpoint\n",
    "AZURE_SEARCH_API_KEY=your-search-key\n",
    "AZURE_OPENAI_ENDPOINT=your-openai-endpoint\n",
    "```\n",
    "\n",
    "## 🚀 Features\n",
    "\n",
    "- **Smart Text Chunking**: Breaks at sentence boundaries when possible\n",
    "- **Metadata Tracking**: Tracks source file, chunk index, and processing dates\n",
    "- **Batch Processing**: Efficiently handles large numbers of files\n",
    "- **Error Handling**: Continues processing even if individual files fail\n",
    "- **Search Optimization**: Vector and semantic search for best results\n",
    "- **Filtering Support**: Search within specific files or date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf0a916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 File Processing Statistics:\n",
      "   Total files: 5\n",
      "   Total chunks: 9\n",
      "\n",
      "📄 Per-file breakdown:\n",
      "   Earth_At_Night_Overview.txt:\n",
      "     📋 Chunks: 2\n",
      "     📏 Total chars: 1,401\n",
      "     📐 Avg chunk: 700 chars\n",
      "     💾 File size: 1,210 bytes\n",
      "   Earth_Night_Ecosystems.txt:\n",
      "     📋 Chunks: 1\n",
      "     📏 Total chars: 883\n",
      "     📐 Avg chunk: 883 chars\n",
      "     💾 File size: 888 bytes\n",
      "   Global_Landscape_Earth_At_Night.txt:\n",
      "     📋 Chunks: 4\n",
      "     📏 Total chars: 3,608\n",
      "     📐 Avg chunk: 902 chars\n",
      "     💾 File size: 3,023 bytes\n",
      "   Human_Activity_At_Night.txt:\n",
      "     📋 Chunks: 1\n",
      "     📏 Total chars: 781\n",
      "     📐 Avg chunk: 781 chars\n",
      "     💾 File size: 782 bytes\n",
      "   Night_Imagery_Disaster_Monitoring.txt:\n",
      "     📋 Chunks: 1\n",
      "     📏 Total chars: 884\n",
      "     📐 Avg chunk: 884 chars\n",
      "     💾 File size: 885 bytes\n"
     ]
    }
   ],
   "source": [
    "# Utility functions for file management\n",
    "def get_txt_file_statistics():\n",
    "    \"\"\"Get detailed statistics about processed files\"\"\"\n",
    "    if not documents:\n",
    "        print(\"❌ No documents processed yet\")\n",
    "        return\n",
    "    \n",
    "    # Group by source file\n",
    "    file_stats = {}\n",
    "    for doc in documents:\n",
    "        filename = doc['source_file']\n",
    "        if filename not in file_stats:\n",
    "            file_stats[filename] = {\n",
    "                'chunks': 0,\n",
    "                'total_chars': 0,\n",
    "                'file_size': doc['file_size'],\n",
    "                'processed_date': doc['processed_date']\n",
    "            }\n",
    "        file_stats[filename]['chunks'] += 1\n",
    "        file_stats[filename]['total_chars'] += doc['chunk_length']\n",
    "    \n",
    "    print(\"📊 File Processing Statistics:\")\n",
    "    print(f\"   Total files: {len(file_stats)}\")\n",
    "    print(f\"   Total chunks: {len(documents)}\")\n",
    "    \n",
    "    print(\"\\n📄 Per-file breakdown:\")\n",
    "    for filename, stats in sorted(file_stats.items()):\n",
    "        avg_chunk_size = stats['total_chars'] / stats['chunks']\n",
    "        print(f\"   {filename}:\")\n",
    "        print(f\"     📋 Chunks: {stats['chunks']}\")\n",
    "        print(f\"     📏 Total chars: {stats['total_chars']:,}\")\n",
    "        print(f\"     📐 Avg chunk: {avg_chunk_size:.0f} chars\")\n",
    "        print(f\"     💾 File size: {stats['file_size']:,} bytes\")\n",
    "\n",
    "def search_specific_file(filename, query, top=3):\n",
    "    \"\"\"Search within a specific file\"\"\"\n",
    "    try:\n",
    "        from azure.search.documents import SearchClient\n",
    "        \n",
    "        search_client = SearchClient(\n",
    "            endpoint=search_endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=search_credential\n",
    "        )\n",
    "        \n",
    "        results = search_client.search(\n",
    "            query,\n",
    "            top=top,\n",
    "            filter=f\"source_file eq '{filename}'\"\n",
    "        )\n",
    "        \n",
    "        print(f\"🔍 Search results for '{query}' in {filename}:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\nResult {i}:\")\n",
    "            print(f\"   Chunk {result.get('chunk_index', 'N/A')}\")\n",
    "            print(f\"   Score: {result.get('@search.score', 'N/A'):.3f}\")\n",
    "            print(f\"   Content: {result.get('content', 'N/A')[:200]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Search failed: {e}\")\n",
    "\n",
    "# Show statistics\n",
    "if documents:\n",
    "    get_txt_file_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb5c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
